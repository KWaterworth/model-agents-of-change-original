[
{
	"metadata" : 
	{
		"title": "Automated Interactive Domain-Specific Conversational Agents that Understand Human Dialogs",
		"year": 2023,
		"month": 3,
		"authors": [
			{
				"name": "Yankai Zeng"
			},
			{
				"name": "Abhiramon Rajasekharan"
			},
			{
				"name": "Parth Padalkar"
			},
			{
				"name": "Kinjal Basu"
			},
			{
				"name": "Joaquin Arias"
			},
			{
				"name": "Gopal Gupta"
			}
		],
		"abstract": "Achieving human-like communication with machines remains a classic, challenging topic in the field of Knowledge Representation and Reasoning and Natural Language Processing. While Large Language Models (LLMs) have shown promise in generating human-like sentences for tasks such as question answering, paragraph summarization, and translation, they rely on pattern-matching rather than a true understanding of the semantic meaning of a sentence. As a result, they may generate incorrect responses. To generate an assuredly correct response, one has to “understand” the semantics of a sentence, so that the missing information can be further requested and the correct response computed. To achieve this “understanding”, logic-based (commonsense) reasoning methods such as Answer Set Programming (ASP) are arguably needed. In this paper, we describe the AutoConcierge system that leverages LLMs and ASP to develop a conversational agent that can truly “understand” human dialogs, at least in restricted domains. AutoConcierge is focused on a specific domain—advising users about restaurants in their local area based on their preferences. AutoConcierge will interactively understand a user’s utterances, identify the missing information in them, and request the user via a natural language sentence to provide it. Once AutoConcierge has determined that all the information has been received, it computes a restaurant recommendation based on the user-preferences it has acquired from the human user. AutoConcierge is based on our STAR framework developed earlier, which uses GPT-3 to convert human dialogs into predicates that capture the deep structure of the dialog’s sentence. These predicates are then input into the goal-directed s(CASP) ASP system for performing commonsense reasoning. To the best of our knowledge, AutoConcierge is the first automated conversational agent that can realistically converse like a human and provide help to humans based on truly understanding human utterances.",
		"keywords": "",
		"institutions": [
			{
				"name": "Depart of Computer Science, UT Dallas, USA"
			},
			{
				"name": "IBM T. J. Watson Research Center, NY, USA"
			},
			{
				"name": "CETINIA, Universidad Rey Juan Carlos, Madrid, Spain"
			}
		]
	},
	"overview": "Zeng et al produced an automated conversational agent which can identify missing information within a user’s request, inquire the user in natural language to obtain the missing information, and leverage the logical nature of Answer Set Programming (ASP) to understand the user’s request and provide a suitable restaurant recommendation based on the user’s needs, as identified by the system.",
	"critical_concepts": "Large Language Models (LLMs): These are transformer-based deep learning models like GPT-3, designed to handle natural language processing (NLP) tasks. LLMs are trained on massive datasets and have the capability to generate coherent text and perform various language-related tasks. However, they may struggle with tasks requiring complex reasoning or understanding of context. Answer Set Programming (ASP) and s(CASP): ASP is a logic programming paradigm used for knowledge representation and reasoning. s(CASP) is a specific system within ASP that supports predicates, constraints, and a query-driven execution strategy. It’s particularly adept at handling commonsense reasoning tasks, making it suitable for tasks where logical inference is required. Design Philosophy: The research aims to emulate how humans process dialogs, with a focus on three main phases: converting sentences to knowledge, processing knowledge to draw conclusions, and converting conclusions into a response sentence. This philosophy guides the architecture and development of the conversational agent, AutoConcierge. Translating Sentences to Predicates: GPT-3 is utilized as a semantic parser to translate human sentences into predicates representing their meaning. This process, known as in-context learning, involves providing GPT-3 with examples of sentence-predicate pairs to learn from. Commonsense Reasoning: Once sentences are translated into predicates, the reasoning module processes this knowledge to compute missing information or make recommendations. Commonsense knowledge, represented using ASP, plays a crucial role in ensuring the coherence and reliability of the generated responses. AutoConcierge Response Generation: The final phase involves generating natural language responses based on the identified predicates representing the user’s preferences and the available restaurant information. GPT-3 is again employed to translate these predicates into human-understandable sentences, with additional rephrasing for enhanced naturalness.",
	"domain_task_problem": "It addresses the challenge of achieving human-like communication with machines. They focus on developing a conversational agent, AutoConcierge, that can truly understand human dialogs, particularly in advising users about restaurants, based on their preferences.",
	"motivations_priorities": "The authors’ motivations in conducting their research stem from recognizing the limitations of existing conversational agents, particularly Large Language Models (LLMs) like ChatGPT, in tasks requiring fact-based and knowledge-oriented responses. While LLMs excel in content generation tasks, they lack understanding of meaning and reasoning capabilities, making them unreliable for tasks such as providing accurate information to users.  Understanding Human Dialog: The primary goal is to enable conversational agents to understand human utterances and respond appropriately, addressing human intentions effectively. Integrating Knowledge and Reasoning: The authors aim to overcome the limitations of LLMs by integrating logic-based reasoning methods, such as ASP, to enhance the agent’s ability to generate factually accurate and logically consistent responses. Developing AutoConcierge: The authors focus on building AutoConcierge as a practical application of their proposed framework, showcasing its ability to provide tailored recommendations based on user preferences and constraints. Advancing Evaluation Criteria: Recognizing the need for objective evaluation criteria for conversational agents, the authors propose a set of criteria, including proactivity, explainability, correctness, consistency, and efficiency, to assess the quality of task-driven conversational agents like AutoConcierge.",
	"eval_metrics": "They evaluate the execution efficiency of the system, with a focus on the time taken for the system to generate the next question and to compute a final recommendation. They measured the time taken for the reasoner to find the first partial answer set, as well as finding all partial answer sets. They compare their agent to Bing AI, identifying several ways in which to compare their system to Bing AI: asking for detailed user preferences, completeness of potential answer set, number of responses generated, and tendency to hallucinate when responses cannot be found which match the requirements as specified by the user. Finally, they propose a list of criteria for evaluating goal-oriented conversational agents, including: proactivity, economy, explainability, correctness, consistency, and efficiency.",
	"approach": "The development of the AutoConcierge system is grounded in replicating the natural conversational process. It emulates the human concierge’s role, where sentences are parsed to extract their meaning, which is represented as knowledge. This knowledge is then checked for consistency and correctness using additional commonsense knowledge. The system follows three phases: converting a sentence to knowledge, processing that knowledge to draw conclusions, and finally, converting these conclusions into a response sentence. The architecture consists of three modules: a module for predicate extraction, a reasoning module, and a response generation module. The reasoning module, powered by the s(CASP) system, contains all the necessary commonsense knowledge for generating a response. GPT-3 is employed for predicate extraction and natural language response generation. The CKT, representing the plan to systematically ask for information and make a decision, is used to explore user preferences. The system uses three phases: translating sentences to predicates, commonsense reasoning, and response generation. Once a sentence is translated into predicates, they are input into the reasoning module to compute the missing information or determine a restaurant recommendation. The knowledge base collects nine properties for each restaurant, and these properties, along with additional predicates, are used for GPT-3’s in-context learning. The system’s implementation involves a semantic parser, a reasoner, and a response generator subsystem. The reasoner, based on the s(CASP) system, updates its state as the conversation progresses and generates responses based on the user’s input. AutoConcierge can explain the reason for its generated recommendation, as it has a complete understanding of user preferences and utilizes commonsense reasoning to compute its response.",
	"results": "AutoConcierge outperforms Bing AI on the restaurant recommendation task.",
	"future_work": "1. Enhancing the knowledge representation using existing ontologies like WordNet. 2. Developing a framework based on Large Language Models (LLMs) such as GPT-3 and Answer Set Programming (ASP) to streamline the development process of chatbots like AutoConcierge. 3. Creating a socialbot akin to AutoConcierge capable of engaging in conversations with humans about various topics such as movies, books, sports, and video games, similar to Amazon Alexa socialbots."
},
{
	"metadata" : 
	{
		"title": "SCIBERT: A Pretrained Language Model for Scientific Text",
		"year": 2019,
		"month": 9,
		"authors": [
			{
				"name": "Iz Beltagy"
			},
			{
				"name": "Kyle Lo"
			},
			{
				"name": "Arman Cohan"
			}
		],
		"abstract": "Obtaining large-scale annotated data for NLP  tasks in the scientific domain is challenging and expensive. We release SCIBERT,  a pretrained language model based on BERT (Devlin et al., 2019) to address the lack of high-quality, large-scale labeled scientific data. SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks.  We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/",
		"keywords": "",
		"institutions": [
			{
				"name": "Allen Institute for Artificial Intelligence, Seattle, WA, USA"
			}
		]
	},
	"overview": "The paper introduces SCIBERT, a pre-trained language model specifically tailored for the scientific domain, addressing the challenge of obtaining large-scale annotated data. Leveraging unsupervised pre-training on a corpus of scientific publications, SCIBERT outperforms BERT on various scientific NLP tasks, achieving new state-of-the-art results. The study compares SCIBERT with BERT on tasks like named entity recognition, text classification, and dependency parsing, demonstrating significant improvements. SCIBERT's efficacy stems from its in-domain vocabulary and pre-training on a substantial corpus of scientific text, making it a valuable resource for scientific NLP applications.",
	"critical_concepts": "NLP and Machine Reading: Natural Language Processing (NLP) involves the interaction between computers and human language. Machine Reading is a subset of NLP that focuses on enabling machines to understand and extract information from text, enabling tasks like question answering and summarization. Deep Neural Models: Deep Neural Models are computational models inspired by the structure and function of the human brain, composed of multiple layers of interconnected nodes (neurons). These models are capable of learning complex patterns from data, making them effective for tasks such as image recognition, natural language processing, and reinforcement learning. Pre-Training of Language Models: Pre-training of Language Models involves training a model on a large corpus of text data without specific task supervision. This pre-training phase aims to teach the model to understand the structure and semantics of natural language, enabling it to perform various downstream tasks more effectively through fine-tuning. BERT architecture: BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model architecture introduced by Google. It utilizes a Transformer architecture, which allows for bidirectional processing of input sequences, capturing contextual information effectively. BERT employs a two-task pre-training objective: predicting masked words and predicting sentence relationships. WordPiece tokenization: WordPiece tokenization is a method used for breaking down words into smaller units called subword tokens. It involves building a vocabulary of subword units based on the most frequently occurring sequences of characters in the training data. This approach allows the model to handle out-of-vocabulary words and capture morphological variations more effectively. Domain-specific training: Domain-specific training involves training machine learning models on data that specifically pertains to a particular domain or field of interest, such as medicine, finance, or law. By tailoring the training data to a specific domain, models can learn domain-specific patterns and terminology, leading to improved performance on tasks within that domain.",
	"domain_task_problem": "The core problem the researchers are trying to solve is the lack of high-quality, large-scale annotated data for NLP tasks in the scientific domain. They recognize that obtaining labeled data in scientific domains is challenging and expensive due to the specialized expertise required for annotation. To address this issue, the researchers propose SCIBERT, a pre-trained language model specifically tailored for scientific text. By pre-training SCIBERT on a large corpus of scientific publications, they aim to leverage unsupervised learning techniques to improve performance on downstream scientific NLP tasks.",
	"motivations_priorities": "Addressing the Lack of Annotated Scientific Data: The primary motivation is to tackle the challenge of obtaining large-scale annotated data for NLP tasks in the scientific domain. The researchers acknowledge that acquiring labeled data in scientific domains is difficult and expensive due to the expertise required for quality annotation. Development of SCIBERT: The researchers introduce SCIBERT, a pre-trained language model based on BERT but specifically trained on a large corpus of scientific text. This model aims to fill the gap left by general-domain pre-trained models like BERT and ELMo, which may not perform optimally on scientific text due to their training on non-specialized corpora such as news articles and Wikipedia. Leveraging Unsupervised Pre-training: The study leverages the success of unsupervised pre-training of language models on large corpora. By pre-training SCIBERT on a large multi-domain corpus of scientific publications, the researchers aim to improve its performance on downstream scientific NLP tasks. Evaluation and Performance Improvement: The researchers prioritize evaluating SCIBERT on a suite of scientific NLP tasks, including sequence tagging, sentence classification, and dependency parsing. Their goal is to demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on these tasks, indicating the effectiveness of SCIBERT for scientific text processing. Accessibility of Resources: The researchers make SCIBERT, along with the code and pre-trained models, openly available to the research community. This reflects a commitment to fostering collaboration and facilitating further advancements in scientific NLP.",
	"eval_metrics": "The researchers evaluate their research using F1 scores, a common metric in natural language processing tasks. They compare the performance of SCIBERT with BERT-Base and other models, observing improvements in F1 scores across various domains and tasks. They also assess the impact of finetuning and the use of an in-domain scientific vocabulary on model performance.",
	"approach": "For downstream NLP tasks, the researchers finetune the pretrained BERT models. They use similar architectures, optimization techniques, and hyperparameters as described in the original BERT paper. For tasks like text classification and sequence labeling, they feed the final BERT vectors into linear classification layers. They also experiment with additional components like conditional random fields for sequence labeling tasks and biaffine matrix attention for dependency parsing. The researchers explore the use of BERT as pretrained contextualized word embeddings. They train simple task-specific models on top of frozen BERT embeddings, similar to the approach used with ELMo. This approach involves feeding BERT vectors into BiLSTM layers for text classification and sequence labeling tasks, and utilizing the full model from Dozat and Manning (2017) for dependency parsing. The researchers conduct some experimentation to determine optimal hyperparameters for each task and BERT variant. They optimize cross entropy loss using Adam, with settings such as learning rate and number of epochs chosen based on performance on the development set.",
	"results": "Performance Improvement with SCIBERT: SCIBERT consistently outperforms BERT-Base across various scientific tasks. On average, SCIBERT achieves a significant improvement of +2.11 F1 with finetuning and +2.43 F1 without finetuning compared to BERT-Base. Biomedical Domain Results: In the biomedical domain, SCIBERT shows notable improvements over BERT-Base, with gains of +1.92 F1 with finetuning and +3.59 F1 without finetuning. SCIBERT also achieves new state-of-the-art (SOTA) results on datasets such as BC5CDR, ChemProt, and EBM-NLP. Computer Science Domain Results: SCIBERT also demonstrates superiority over BERT-Base in the computer science domain, with gains of +3.55 F1 with finetuning and +1.13 F1 without finetuning. It achieves new SOTA results on datasets like ACL-ARC and the NER part of SciERC.Multiple Domains Results: Across multiple domains, SCIBERT outperforms BERT-Base with gains of +0.49 F1 with finetuning and +0.93 F1 without finetuning. It also surpasses the previous SOTA on the SciCite dataset.Effect of Finetuning: Finetuning BERT leads to improved results across all scientific domains, with the most significant effects observed in computer science and biomedical tasks. Finetuning generally outperforms using task-specific architectures atop frozen embeddings.Effect of SCIVOCAB: SCIBERT benefits from its in-domain scientific vocabulary (SCIVOCAB), resulting in an average improvement of +0.60 F1 compared to using the base vocabulary (BASEVOCAB). This improvement is consistent across different scientific domains.",
	"future_work": "For future endeavors, the researchers plan to release a SCIBERT variant comparable to BERT-Large and explore varying proportions of papers from different domains. Given the resource-intensive nature of training such language models, their goal is to create a unified resource which is useful across diverse domains, aiming for efficiency and broad applicability."
},
{
	"metadata" : 
	{
		"title": "Reliable Natural Language Understanding with Large Language Models and Answer Set Programming",
		"year": 2023,
		"month": 0,
		"authors": [
			{
				"name": "Abhiramon Rajasekharan"
			},
			{
				"name": "Yankai Zeng"
			},
			{
				"name": "Parth Padalkar"
			},
			{
				"name": "Gopal Gupta"
			}
		],
		"abstract": "Humans understand language by extracting information (meaning) from sentences, combining it with existing commonsense knowledge, and then performing reasoning to draw conclusions. While large language models (LLMs) such as GPT-3 and ChatGPT are able to leverage patterns in the text to solve a variety of NLP tasks, they fall short in problems that require reasoning. They also cannot reliably explain the answers generated for a given question. In order to emulate humans better, we propose STAR, a framework that combines LLMs with Answer Set Programming (ASP). We show how LLMs can be used to effectively extract knowledge—represented as predicates—from language. Goal-directed ASP is then employed to reliably reason over this knowledge. We apply the STAR framework to three different NLU tasks requiring reasoning: qualitative reasoning, mathematical reasoning, and goal-directed conversation. Our experiments reveal that STAR is able to bridge the gap of reasoning in NLU tasks, leading to significant performance improvements, especially for smaller LLMs, i.e., LLMs with a smaller number of parameters. NLU applications developed using the STAR framework are also explainable: along with the predicates generated, a justification in the form of a proof tree can be produced for a given output.",
		"keywords": "",
		"institutions": [
			{
				"name": "University of Texas at Dallas, Richardson, USA"
			}
		]
	},
	"overview": "This research introduces the STAR (Semantic-parsing Transformer and ASP Reasoner) framework, aiming to enhance natural language understanding (NLU) systems by incorporating explicit reasoning capabilities. The motivation stems from the limitations of large language models (LLMs) like GPT-3 in tasks requiring complex reasoning and reliable explanations.  The STAR framework combines LLMs for semantic parsing with Answer Set Programming (ASP) for reasoning. It maps natural language sentences to predicates representing their semantics, augments them with commonsense knowledge, and uses ASP to perform reasoning and draw conclusions.  The paper applies the STAR framework to three NLU tasks: qualitative reasoning, mathematical reasoning, and goal-directed conversation. Experiments demonstrate that STAR improves answer prediction accuracy, especially for smaller LLMs, and provides explainable outputs through proof trees. In tasks involving in-depth reasoning, such as a hotel concierge conversation bot, STAR outperforms LLM-only approaches by providing more reliable suggestions and handling larger databases.",
	"critical_concepts": "Natural Language Understanding (NLU): NLU encompasses the study of enabling computers to comprehend and interact with human language. It involves tasks like understanding the meaning of sentences, grasping context, resolving ambiguities, and providing explanations for understanding language better. Large Language Models (LLMs): LLMs are advanced deep learning models, such as GPT-3, designed to understand and generate human-like text. They leverage transformer architectures and vast amounts of training data to excel in tasks like reading comprehension, semantic parsing, and translation. However, they may struggle with complex reasoning tasks due to limitations in their training data and architecture. Semantic Parsing: Semantic parsing involves converting natural language sentences into structured representations, such as logical forms or predicates, that capture the meaning or semantics of the sentences. This process enables computers to understand and manipulate language in a more structured and systematic manner. Answer Set Programming (ASP): ASP is a logical programming paradigm used for knowledge representation and reasoning. It involves defining facts, rules, and constraints to model a problem domain, and then using a solver, such as s(CASP), to compute solutions or answers based on the provided knowledge. Commonsense Knowledge Representation: This involves encoding commonsense knowledge, which includes general knowledge and intuition about the world, into a formal representation suitable for computational reasoning. ASP can be used to represent commonsense knowledge using facts, rules, default rules, integrity constraints, and multiple possible worlds. Qualitative Reasoning: Qualitative reasoning involves reasoning about the qualitative relationships between properties of objects and events, such as friction, heat, speed, and time, without precise numerical values. It focuses on understanding and drawing inferences based on observations and qualitative information. QuaRel Dataset: The QuaRel dataset contains questions designed to test qualitative reasoning abilities, particularly related to physical properties. It provides a set of questions along with associated logical forms that capture the semantics of the questions, serving as a benchmark for evaluating models' performance in qualitative reasoning tasks. Fine-Tuning and In-Context Learning: Fine-tuning and in-context learning are machine learning techniques used to adapt pre-trained models, like LLMs, to specific tasks or datasets. Fine-tuning involves updating the parameters of the pre-trained model using task-specific data, while in-context learning involves providing task-specific examples along with the context to the model for learning. Experimental Design and Evaluation: This refers to the process of designing experiments to evaluate the performance of models systematically. It involves defining metrics, selecting datasets, designing tasks, running experiments, and analyzing results to assess the effectiveness of the models. Statistical Analysis: Statistical analysis involves using mathematical and statistical methods to analyze experimental results and draw conclusions. It includes measures such as accuracy, precision, recall, and F1-score to quantify the performance of models and compare different approaches.",
	"domain_task_problem": "The paper addresses the task of solving word problems in algebra, specifically focusing on addition and subtraction problems. It aims to develop a framework called STAR (Semantic-parsing Transformer and ASP Reasoner) for solving such problems by leveraging natural language understanding (NLU) techniques, Large Language Models (LLMs), semantic parsing, and Answer Set Programming (ASP). The framework involves converting word problems into structured representations using predicates, reasoning over them using ASP, and generating answers. Additionally, the paper explores the application of the STAR framework in developing a conversational bot for restaurant recommendations, demonstrating its versatility in handling complex commonsense reasoning tasks in natural language dialogue.",
	"motivations_priorities": "The authors are motivated by the overarching goal of achieving human-level natural language understanding (NLU) in AI systems. They identify the complexity involved in understanding language, which includes proficiency in language interpretation, context understanding, common-sense reasoning, question generation, and explanation generation. They recognize that while Large Language Models (LLMs) have shown proficiency in various language tasks, they often fall short in tasks requiring complex reasoning, such as commonsense reasoning or solving mathematical word problems.",
	"eval_metrics": "Dataset Selection: They use a dataset comprising 91 word problems selected from a collection provided by Koncel-Kedziorski et al. The problems are chosen based on their suitability for encoding logic and reasoning. Model Training: They utilize the text-davinci-003 model, the most capable GPT-3 variant for in-context learning. They provide a context containing a few problems and their corresponding predicates to the GPT-3 model to generate facts and query predicates for new problems. Experimental Process: Initially, they start with a smaller context of 12 problems and analyze the mistakes made by the LLM in predicate generation. They iteratively refine the context by adding more problems to address these mistakes, resulting in a final context of 24 problems. They then compare the performance of the baseline model (text-davinci-003-Direct) with their approach (text-davinci-003-STAR). Performance Metrics: They measure the accuracy of both models on a test set of 67 problems. Both the baseline model and their STAR approach achieve 100% accuracy. However, they highlight that their STAR approach offers the additional advantage of generating justifications, making it explainable. Justification Tree Example: They provide an example of a justification tree generated by the s(CASP) system for a specific problem, demonstrating how the computed answer is derived from the generated predicates. Discussion on Weaknesses and Future Work: The authors acknowledge that while their approach achieves high accuracy on simple algebraic word problems, more complex problems may reveal performance differences. They plan to explore this aspect further in future work. Comparison with Direct GPT-3 Conversation: They compare their conversational bot approach with direct GPT-3 conversation generation. They highlight that while GPT-3 can generate natural-sounding sentences, it lacks reliability and understanding of the given knowledge. In contrast, bots developed using their STAR framework exhibit explicit commonsense reasoning, avoiding such problems.",
	"approach": "Therefore, the authors prioritize the development of a framework called STAR (Semantic-parsing Transformer and ASP Reasoner), which aims to mimic human-like language understanding. The STAR framework maps natural language sentences to semantic representations, augments them with commonsense knowledge, and employs explicit reasoning to draw conclusions, akin to human cognitive processes. The framework leverages LLMs for semantic parsing and shifts the reasoning burden to an Answer Set Programming (ASP) system. This approach aims to address the limitations of LLMs in complex reasoning tasks and provide more reliable and explainable NLU systems.",
	"results": "Performance of the STAR Framework: The STAR framework consistently outperforms purely LLM-based approaches in qualitative reasoning tasks. It achieves advancements in performance on datasets like QuaRel, particularly showing significant improvement for smaller LLMs like Curie. Furthermore, STAR can generate justification trees to explain its reasoning process across all tasks. Concierge Bot Application: A comparison between the STAR-based approach and an LLM-only approach for developing a concierge bot reveals that the STAR-based approach provides more reliable and faithful suggestions based on the information in the restaurant database. Additionally, it enables long, interactive, and meaningful conversations. Potential Applications: The STAR framework has broad potential applications beyond the tasks explored in the research. It can be applied to any NLU application that requires reasoning about textual knowledge or utterances. Examples include automatically extracting formal software requirements from textual specifications, building conversational agents for various domains, and enhancing machine translation reliability.",
	"future_work": "The authors plan to further improve the performance of the STAR framework, especially for problems requiring complex reasoning. They aim to develop a general commonsense knowledge base that applications built using the framework can leverage, expanding its applicability and effectiveness across different domains and tasks."
},
{
	"metadata" : 
	{
		"title": "Braid: Weaving Symbolic and Neural Knowledge into Coherent Logical Explanations",
		"year": 2021,
		"month": 12,
		"authors": [
			{
				"name": "Aditya Kalyanpur"
			},
			{
				"name": "Thomas Breloff"
			},
			{
				"name": "David Ferrucci"
			}
		],
		"abstract": "Traditional symbolic reasoning engines, while attractive for their precision and explicability, have a few major drawbacks: the use of brittle inference procedures that rely on exact matching (unification) of logical terms, an inability to deal with uncertainty, and the need for a precompiled rule-base  of knowledge (the “knowledge acquisition” problem). To ad dress these issues, we devise a novel logical reasoner called  Braid, that supports probabilistic rules, and uses the notion of custom unification functions and dynamic rule generation to overcome the brittle matching and knowledge-gap problem prevalent in traditional reasoners. In this paper, we describe  the reasoning algorithms used in Braid, and their implementa tion in a distributed task-based framework that builds proof/  explanation graphs for an input query. We use a simple QA example from a children’s story to motivate Braid’s design and explain how the various components work together to produce a coherent logical explanation. Finally, we evaluate  Braid on the ROC Story Cloze test and achieve close to state- of-the-art results while providing frame-based explanations.",
		"keywords": "",
		"institutions": [
			{
				"name": "Elemental Cognition Inc."
			}
		]
	},
	"overview": "The research introduces Braid, a novel logical reasoner designed to address the limitations of traditional symbolic reasoning engines. Unlike traditional systems, Braid supports probabilistic rules, dynamic rule generation, and custom unification functions, enabling it to handle uncertainty and overcome the knowledge acquisition problem. By combining symbolic reasoning with statistical methods, Braid constructs coherent logical explanations for queries, demonstrated through examples from children's stories. It employs a distributed task-based framework to build proof/explanation graphs and achieves close-to-state-of-the-art results on the ROC Story Cloze test while providing frame-based explanations. Braid's components, including custom unifiers and dynamic rule generators, work synergistically to produce deep logical explanations, showing promising results in story understanding tasks.",
	"critical_concepts": "Symbolic Reasoning Engines: Traditional symbolic reasoning engines are precise but have limitations such as brittle inference procedures, inability to deal with uncertainty, and the need for precompiled rule-bases. Brittle Matching and Knowledge Gap Problem: Traditional reasoning engines suffer from brittle matching and the knowledge acquisition problem, which refers to the challenge of acquiring the rules necessary for reasoning. Hand-coded rules are not scalable. Neuro-Symbolic Approaches: These approaches combine neural networks with symbolic reasoning to address the limitations of traditional symbolic reasoning engines. Examples include NL-Prolog, Logic Tensor Networks, and IBM's Logic NNs. Braid: Braid is a novel logical reasoner that addresses the limitations of traditional symbolic reasoning engines by supporting probabilistic rules, custom unification functions, and dynamic rule generation. It combines symbolic reasoning with statistical methods to overcome brittle matching and knowledge gap problems. Dynamic Rule Generation (DRG): DRG provides missing rule knowledge on the fly to the reasoning engine. In this research, a neural DRG implementation uses a pre-trained transformer model fine-tuned on a dataset of common-sense inference rules. Fuzzy Unification: Fuzzy unification is used to align logical propositions that may not match exactly but share similarities. It considers word/phrase similarity to boost the match score. Provers: Provers are functions that perform reasoning steps by unifying goals with clauses in the knowledge base. In this research, specialized provers are used for different reasoning tasks, such as explaining agentful actions in stories. Distributed Proof Graph Builder: Braid uses a distributed task-based framework to construct proof/explanation graphs for input queries. This architecture enables parallelization and scalability for reasoning tasks. Proof Finding and Ranking using ILP: Braid formulates proof-finding as an Integer Linear Programming (ILP) problem, where the goal is to extract a ranked list of valid proofs for query answers based on the confidence of included nodes in the proof graph. Template of Understanding: The Template of Understanding is used to test an AI system's deep understanding of a narrative story. It includes questions that require implicit background knowledge and logical reasoning to answer accurately.",
	"domain_task_problem": "The researchers solve the problem of understanding and generating plausible story endings in the domain of narrative comprehension. Specifically, they tackle the ROC Story Cloze Test, a task where participants are presented with a brief narrative consisting of four sentences and asked to select the most appropriate ending from two choices. This task requires not only comprehending the context and themes presented in the story but also applying common sense and narrative coherence to determine the most plausible conclusion. By addressing this task, the researchers aim to develop a system capable of effectively understanding and generating coherent story endings, thereby advancing the field of natural language understanding and narrative reasoning.",
	"motivations_priorities": "Overall, the authors are motivated to advance the state of the art in NLU by developing a reasoning framework that combines the best aspects of symbolic and statistical approaches to overcome the limitations of existing systems.They focus on addressing the limitations of traditional symbolic reasoning engines in natural language understanding (NLU). They aim to overcome challenges such as brittle inference procedures, the inability to deal with uncertainty, and the knowledge acquisition problem inherent in these systems.To tackle these issues, the authors propose a novel logical reasoner called Braid, which supports probabilistic rules and utilizes custom unification functions and dynamic rule generation. Braid aims to combine the precision and explicability of symbolic reasoning engines with the flexibility and adaptability of statistical methods.  The motivation for developing Braid comes from the recognition of the limitations of existing neuro-symbolic approaches, such as NL-Prolog, which lack full transparency in explanations and have fixed embeddings for entities/predicates at test time.They emphasize the importance of transparent and interpretable explanations in logical reasoning tasks, especially in understanding complex narratives.",
	"eval_metrics": "The researchers evaluated the effectiveness of their approach using the ROC Story Cloze Test. Here's a summary of their evaluation metrics:  Task Description: The ROC Story Cloze Test presents a scenario where participants are given a 4-sentence story and asked to choose the most plausible ending from two options. This task relies on commonsense knowledge and requires understanding the context of the story to select the appropriate ending. Baseline Comparison: The researchers compared their approach with existing state-of-the-art (SOTA) systems, including E2E neural models, HintNet, GPT2, ISCK, and BERT-based models, which achieved high accuracies (up to 90.6%) on this task. However, these models lack explicability. Approach: The researchers developed a hybrid neuro-symbolic solution using Braid. Their approach involved frame inference, where they aimed to detect the applicable frame from the first four sentences of the story. They hypothesized that identifying the frame could help predict the right ending consistent with the detected frame and provide an explanation for the chosen ending. Frame Inference: They decomposed the Story Cloze problem into two steps: detecting a frame given the story and computing the probability of each ending being the right ending considering the detected frame. Classifier Training: They trained a classifier using a pre-trained encoder-decoder model (T5-base) to predict the corresponding frame given the first four sentences of the story. The classifier achieved high accuracy (87.5%) on the dev set. Experiment Setup: They conducted several experiments using Braid to solve the Story Cloze task, including variations where they used textual versions or semantic parse versions of neural dynamic rule generators (DRGs) fine-tuned on different models (T5-base or Glucose).",
	"approach": "The Braid framework is a parallelized infrastructure designed for constructing deductive proof/explanation graphs for a given query and knowledge base (KB). The approach comprises two key functions: Unifiers and Provers.  Unifiers: Unification is a core function in any first-order logic (FOL)-based reasoner. Braid's Unifiers generalize the notion of unification to be any FOL formulae matching function. Instead of relying solely on syntactic unification, Braid's Unifiers take into account additional context from the KB to boost matching scores. For example, Braid utilizes a custom unification function that considers word/phrase similarity to align different propositions, even if they use different predicates and arguments. Provers: Braid's Provers construct a proof graph by using unification methods to backchain on clauses (rules/facts) in the KB. These Provers support various reasoning algorithms and operate locally, allowing for parallelization across cores and machines. Each Prover performs a single-step expansion of the graph along a particular reasoning path. Default Backchaining Prover (SLD+): Based on SLD resolution, this Prover extends the standard SLD algorithm with custom unifier functions and dynamic rule generation. It splits conjunctive goals into individual conjuncts for parallel evaluation, dynamically generates rules, and creates additional nodes for rule consequents containing existentially quantified variables. Agentful Action Prover: Specialized for explaining actions carried out by agents in narratives, this Prover follows a reasoning strategy of first finding the motivations of the agent and then checking if the action leads to one of the agent's objectives. It uses a logical rule to infer motivations and utilizes dynamic rule generation for efficiency. Dynamic Rule Generator (DRG): Braid's DRG provides missing rule knowledge on the fly to the reasoning engine. It takes a target goal and KB as input and returns relevant rules for proving the goal. Braid implements a neural DRG based on the GLUCOSE dataset, which consists of contextual common-sense inference rules learned from children's stories. Proof Finding and Ranking using ILP: Once the Braid Master has built the proof graph, the final step is to extract a ranked list of valid proofs for the query answers. Braid formulates this as an Integer Linear Programming (ILP) problem, where each solution to the ILP problem constitutes a single proof-tree for the input query. By integrating these components, Braid aims to provide deep logical explanations for NLU tasks, particularly in understanding complex narratives. It combines symbolic reasoning with statistical methods and leverages dynamic rule generation to address the challenges of uncertainty and knowledge acquisition in traditional reasoning engines.",
	"results": "The results of their experiments showed that their approach using Braid achieved competitive performance compared to existing SOTA systems while providing frame-based explanations. The accuracies ranged from 87.17% to 89.88%, demonstrating the effectiveness of their hybrid neuro-symbolic solution.",
	"future_work": ""
},
{
	"metadata" : 
	{
		"title": "From Statistical Relational to Neuro-Symbolic Artificial Intelligence",
		"year": 2020,
		"month": 3,
		"authors": [
			{
				"name": "Luc De Raedt"
			},
			{
				"name": "Sebastijan Dumanˇci ́c"
			},
			{
				"name": "Robin Manhaeve"
			},
			{
				"name": "Giuseppe Marra"
			}
		],
		"abstract": "Neuro-symbolic and statistical relational artificial  intelligence both integrate frameworks for learning with logical reasoning. This survey identifies several parallels across seven different dimensions between these two fields. These cannot only be used to characterize and position  neuro-symbolic artificial intelligence approaches  but also to identify a number of directions for further research.",
		"keywords": "",
		"institutions": [
			{
				"name": "KU Leuven, Department of Computer Science"
			}
		]
	},
	"overview": "The paper presents a comprehensive taxonomy of neuro-symbolic models, categorizing them based on seven dimensions including directed vs. undirected, grounding vs. proofs, logic vs. probability vs. neural, parameter vs. structure learning, symbols vs. sub-symbols, type of logic, and open challenges. Through this taxonomy, the paper offers a systematic overview of the field, highlighting key concepts and areas for further research and development in neuro-symbolic computing.",
	"critical_concepts": "Taxonomy of Neuro-Symbolic Models: The paper categorizes neuro-symbolic models based on seven dimensions, including directed vs. undirected, grounding vs. proofs, logic vs. probability vs. neural, parameter vs. structure learning, symbols vs. sub-symbols, type of logic, and open challenges. Directed vs. Undirected Models: Drawing from graphical model theory, the distinction between directed and undirected models is explored, highlighting their differences in expressing causal relationships and soft constraints among variables. Grounding vs. Proofs: The paper discusses the model-theoretic and proof-theoretic perspectives to inference, emphasizing the importance of grounding (replacing variables with constants) and proofs (sequence of inference steps) in neuro-symbolic computing. Logic, Probability, and Neural Integration: It addresses how neuro-symbolic models integrate logic, probability, and neural methods, with some approaches focusing more on logic, some on probability, and others on neural networks, with varying degrees of integration between them. Parameter vs. Structure Learning: The distinction between learning the structure of models (e.g., logical clauses) and learning the parameters (e.g., probabilities or weights) associated with those structures is highlighted. Symbols vs. Sub-symbols: The paper discusses the representation of entities as symbols or sub-symbols (vectorized representations) and how these representations are used for reasoning. Type of Logic: It categorizes approaches based on the type of logic they extend or utilize, including propositional logic, relational logic, first-order logic, or logic programs (as in Prolog). Open Challenges: Finally, the paper outlines several challenges in neuro-symbolic computing, such as probabilistic reasoning, structure learning, scaling inference, data efficiency, and symbolic representation learning, which are crucial for further research and development in the field.",
	"domain_task_problem": "Identifying a set of seven dimensions that the fields of statistical relational artificial intelligence and neuro-symbolic ai have in common, which can be used to categorize both StarAI and NeSy approaches",
	"motivations_priorities": "The authors are motivated by the lack of interaction between the fields of neuro-symbolic computation (NeSy) and statistical relational learning and artificial intelligence (StarAI), despite their common interest in combining logic or symbolic reasoning with learning paradigms such as probabilistic graphical models or neural networks. They aim to stimulate more cross-fertilization between these fields by identifying similarities and common challenges. The discrepancy between the two fields serves as the key motivation for their survey, which aims to point out these similarities and stimulate further research. They identify seven dimensions common to both fields and use them to categorize approaches in both StarAI and NeSy. By positioning various systems along these dimensions and pointing out analogies between them, they aim to identify opportunities for further research. Despite the differences between StarAI and NeSy, such as operating at different levels of abstraction, the authors focus on a logical and probabilistic perspective inherited from StarAI, while also considering developments consistent with this perspective in NeSy. They aim for a representative overview of systems and approaches rather than aiming for completeness due to page limitations.",
	"eval_metrics": "",
	"approach": "The authors create a taxonomy, which provides a structured framework for understanding and categorizing NeSy models based on key characteristics and challenges.  They explore and categorize various approaches in neuro-symbolic computing based on seven dimensions.",
	"results": "The paper provides a taxonomy of neuro-symbolic models based on these dimensions, offering a comprehensive overview of the field and highlighting areas for further research and development.",
	"future_work": ""
},
{
	"metadata" : 
	{
		"title": "Neuro-Symbolic AI for Compliance Checking of Electrical Control Panels",
		"year": 2023,
		"month": 5,
		"authors": [
			{
				"name": "Vito Barbara"
			},
			{
				"name": "Massimo Guarascio"
			},
			{
				"name": "Nicola Leone"
			},
			{
				"name": "Giuseppe Manco"
			},
			{
				"name": "Alessandro Quarta"
			},
			{
				"name": "Francesco Ricca"
			},
			{
				"name": "Ettore Ritacco"
			}
		],
		"abstract": "Artificial Intelligence plays a main role in supporting and improving smart manufacturing and Industry 4.0, by enabling the automation of different types of tasks manually performed by domain experts. In particular, assessing the compliance of a product with the relative schematic is a time-consuming and prone-to-error process. In this paper, we address this problem in a specific industrial scenario. In particular, we define a Neuro-Symbolic approach for automating the compliance verification of the electrical control panels. Our approach is based on the combination of Deep Learning techniques with Answer Set Programming (ASP), and allows for identifying possible anomalies and errors in the final product even when a very limited amount of training data is available. The experiments conducted on a real test case provided by an Italian Company operating in electrical control panel production demonstrate the effectiveness of the proposed approach.",
		"keywords": "Automated Quality Control Systems, Answer Set Programming, Computer Vision, Data Scarcity",
		"institutions": [
			{
				"name": "University of Calabria, Italy"
			},
			{
				"name": "ICAR-CNR, Italy"
			},
			{
				"name": "Sapienza University of Rome, Italy"
			},
			{
				"name": "University of Udine, Italy"
			}
		]
	},
	"overview": "This research proposes an innovative approach to automate compliance verification processes for control panels in industrial settings. Addressing the challenge of understanding image contents and extracting constraints encoded in schematics, the authors devise a framework comprising two main modules: Component Detection and Quality Assessment. The Component Detection module utilizes deep learning, specifically Mask R-CNN, to recognize electrical components within control panel images. Synthetic data generation techniques are employed to overcome the scarcity of labeled data, enhancing the model's robustness. The Quality Assessment module employs Answer Set Programming (ASP) to compare control panel schematics with neural network outputs, identifying any anomalies. Experimental results demonstrate the effectiveness of the deep learning-based detection model in recognizing components and the scalability of the ASP-based compliance checking module, showcasing promising performance for real-world applications in industrial compliance verification.",
	"critical_concepts": "Control Panels: Familiarity with the structure and components of control panels used in industrial settings is essential. This includes understanding the types of components typically found in control panels and their arrangement. Compliance Verification: Knowledge of compliance verification processes in industrial contexts is necessary. This involves ensuring that constructed control panels adhere to design specifications and regulatory requirements. Image Processing and Computer Vision: Understanding basic concepts of image processing and computer vision is crucial for comprehending how the authors approach the problem of automating compliance verification using image data. This includes techniques for image preprocessing, segmentation, and object detection. Deep Learning (DL): Familiarity with deep learning techniques, particularly convolutional neural networks (CNNs), is important for understanding how DL models are used for component detection in control panel images. Synthetic Data Generation: Knowledge of synthetic data generation techniques is helpful for understanding how the authors address the challenge of limited labeled data. Synthetic data generation involves creating artificial data samples that resemble real data, which can be used to train machine learning models. Answer Set Programming (ASP): Understanding the basics of ASP is necessary for comprehending how the authors perform compliance checking. ASP is a declarative programming paradigm used for solving search problems, and it involves encoding problem constraints and rules in logic programs. Precision, Recall, and IoU: Understanding evaluation metrics such as precision, recall, and Intersection over Union (IoU) is important for interpreting the performance of the proposed approach. These metrics are commonly used in object detection tasks to assess the accuracy and completeness of predictions. Experimental Design and Evaluation Metrics: Familiarity with experimental design principles and evaluation metrics used in machine learning research is crucial for interpreting the results presented in the paper. This includes understanding how training and test datasets are constructed, as well as the metrics used to measure the performance of the proposed approach.",
	"domain_task_problem": "The authors are addressing the task of automating the compliance verification process of control panels in industrial settings. This involves verifying whether the components assembled in control panels conform to their schematics. The domain is industrial automation, particularly focused on ensuring compliance with design specifications in control panel manufacturing. The problem entails dealing with challenges such as understanding image contents, extracting constraints encoded in schematics, and managing the lack of labeled data and unlabeled data distribution.",
	"motivations_priorities": "The paper addresses the challenge of automating compliance verification in the manufacturing industry, focusing on electrical control panels (ECPs). Manual verification processes are time-consuming and error-prone, motivating the development of AI-based solutions. The authors propose a Neuro-Symbolic approach combining Deep Learning (DL) with Answer Set Programming (ASP) to identify anomalies in ECPs' final products, aiming for high accuracy and real-time performance. Their innovative solution addresses issues such as data scarcity and custom designs, offering a systematic approach to integrate learning and reasoning, contributing to the emerging field of neuro-symbolic AI for industrial quality control.",
	"eval_metrics": "The evaluation metrics used in the experiments include Precision, Recall, Precision-Recall Curve, Intersection Over Union (IoU), Average Precision (AP), Average Recall (AR), mean Average Precision (mAP), and mean Average Recall (mAR).  Precision (p) and Recall (r) are standard metrics defined as the ratio of true positives (TP) to the sum of true positives and false positives (FP) for precision, and the ratio of true positives to the sum of true positives and false negatives (FN) for recall. Precision-Recall Curve: Precision is plotted against recall for different threshold values, representing the model's ability to identify target objects. Intersection Over Union (IoU): IoU measures the fraction of the overlapping area between the ground truth bounding box and the predicted bounding box. Average Precision (AP) and Average Recall (AR) are calculated by integrating precision and recall over all possible thresholds. Mean Average Precision (mAP) and Mean Average Recall (mAR) are obtained by averaging AP and AR over all class components, providing overall performance measures for object detection tasks. These metrics are used to assess the capability of the proposed approach in detecting components installed in Electrical Control Panels (ECPs) using both synthetic and real images.",
	"approach": "The paper proposes a framework for automating the compliance verification process of control panels, consisting of two main modules: Component Detection and Quality Assessment. The Component Detection module utilizes deep learning techniques, specifically Mask R-CNN, to recognize electrical components within the panels from images. To address challenges such as data scarcity and lack of labeling annotations, the authors employ synthetic data generation methods and data augmentation strategies. The Quality Assessment module uses Answer Set Programming (ASP) to compare the reconstructed panel scheme from the neural network output with the original schematic, identifying any anomalies. The ASP program encodes compliance rules and optimizes the mapping between components from the neural network and the schematic, considering both relative position and distance. Overall, the approach integrates deep learning for object detection with reasoning using ASP, offering a systematic solution for compliance verification in industrial settings.",
	"results": "The results of the system showcase high effectiveness in both the deep learning-based component detection model and the scalability of the ASP-based compliance checking module. The deep learning model achieves optimal performances with a mean average precision (mAP) of 0.954 and mean average recall (mAR) of 0.935, demonstrating its capability to accurately recognize components in control panel images. Furthermore, the ASP-based compliance checking module exhibits efficient performance, providing answers in a short time, even for instances with a large number of components.  The results of the experiments are discussed in terms of the effectiveness of the DL-based detection model and the scalability of the ASP module.  DL-Based Detection Model: The detection model exhibited optimal performances with a mean average precision (mAP) of 0.954 and a mean average recall (mAR) of 0.935. Precision-recall curve analysis at a fixed Intersection Over Union (IoU) threshold (θ = 0.5) showed a resulting area of 0.947, indicating good performance. Detailed precision-recall curves for each instance highlighted the model's accuracy in recognizing different types of components, except for rare cases with slightly lower quality due to factors like inaccurate image acquisition. ASP Module Scalability: The execution time of the ASP-based component was measured across instances of compliance testing. Instances ranged from 6 to 50 labels (types of components) and from 12 to 75 components. The ASP engine DLV2 provided answers in a short time, averaging around 1.93 seconds for real-world sized instances and with acceptable performance (maximum about 18 seconds) even for instances with 75 components. Overall, the results indicate that both the DL-based detection model and the ASP module demonstrate effectiveness and scalability, making the proposed approach suitable for real-world industrial scenarios.",
	"future_work": "For future work, the authors suggest several avenues for improvement. These include enhancing the deep learning model's robustness to factors like inaccurate image acquisition and incorporating additional perspectives of components in the catalog. Moreover, they propose exploring advanced optimization techniques to further improve the efficiency of the ASP-based compliance checking module. Additionally, extending the research to address more complex industrial scenarios and integrating real-time monitoring capabilities are among the potential directions for future research."
},
{
	"metadata" : 
	{
		"title": "Semantic Probabilistic Layers for Neuro-Symbolic Learning",
		"year": 2022,
		"month": 0,
		"authors": [
			{
				"name": "Kareem Ahmed"
			},
			{
				"name": "Stefano Teso"
			},
			{
				"name": "Kai-Wei Chang"
			},
			{
				"name": "Guy Van den Broeck"
			},
			{
				"name": "Antonio Vergari"
			}
		],
		"abstract": "We design a predictive layer for structured-output prediction (SOP) that can be plugged into any neural network guaranteeing its predictions are consistent with a set of predefined symbolic constraints. Our Semantic Probabilistic Layer (SPL) can model intricate correlations, and hard constraints, over a structured output space all while being amenable to end-to-end learning via maximum likelihood. SPLs combine exact probabilistic inference with logical reasoning in a clean and modular way, learning complex distributions and restricting their support to solutions of the constraint. As such, they can faithfully, and efficiently, model complex SOP tasks beyond the reach of alternative neuro-symbolic approaches. We empirically demonstrate that SPLs outperform these competitors in terms of accuracy on challenging SOP tasks including hierarchical multi-label classification, pathfinding and preference learning, while retaining perfect constraint satisfaction. Our code is made publicly available on Github at github.com/KareemYousrii/SPL.",
		"keywords": "",
		"institutions": [
			{
				"name": "CS Department UCLA"
			},
			{
				"name": "CIMeC and DISI University of Trento"
			},
			{
				"name": "School of Informatics University of Edinburgh"
			}
		]
	},
	"overview": "The research focuses on advancing neuro-symbolic computing by introducing Symbolic Propagation Layers (SPLs), a novel framework that seamlessly integrates complex probabilistic reasoning and logical constraints into neural network classifiers. By incorporating SPLs, the researchers aim to address the challenge of efficiently solving neuro-symbolic symbolic optimization problems (SOPs), such as pathfinding and hierarchical multi-label classification (HMLC). Their work significantly improves the state-of-the-art performance on these challenging tasks, offering a clear interface for integrating logical constraints and probabilistic reasoning while retaining efficiency in both inference and training processes. Moreover, the researchers envision future extensions of SPLs to incorporate logical constraints across multiple networks, potentially automating constraint learning from data and transparently integrating them into large language models, thus equipping them with scalable logical reasoning capabilities.",
	"critical_concepts": "Neuro-symbolic Computing: This refers to the integration of symbolic reasoning and neural networks, aiming to combine the strengths of both approaches to tackle complex problems. Symbolic Propagation Layers (SPLs): These are a novel framework introduced in the research, allowing for the seamless integration of complex probabilistic reasoning and logical constraints into neural network classifiers. Understanding how SPLs work and their role in neuro-symbolic computing is crucial. Probabilistic Reasoning: This involves making predictions or decisions in the presence of uncertainty, often using probability theory to model and quantify uncertainty. Logical Constraints: These are logical statements or rules that restrict the possible solutions to a problem. In the context of this research, understanding how logical constraints are formulated and integrated into neural networks is essential. Neural Network Architectures: Knowledge of neural network architectures, such as ResNet and MLP, is necessary to understand how SPLs are applied and evaluated in the experiments. Optimization Problems: Pathfinding and hierarchical multi-label classification (HMLC) are examples of optimization problems addressed in the research. Understanding the nature of these problems and how neural networks can be applied to solve them is critical. Evaluation Metrics: Metrics such as exact match, Hamming score, and consistency are used to evaluate the performance of SPLs compared to baseline approaches. Understanding these metrics and their significance is essential for interpreting the experimental results. Application Domains: The research applies SPLs to various application domains, including pathfinding and HMLC. Familiarity with these domains and the specific challenges they pose is necessary to understand the context and significance of the research findings.",
	"domain_task_problem": "The researchers are working within the domain of neuro-symbolic computing, aiming to address the challenge of integrating complex probabilistic reasoning and logical constraints into neural network classifiers. Specifically, they are focusing on solving tasks related to neuro-symbolic symbolic optimization problems (SOPs), including pathfinding and hierarchical multi-label classification (HMLC).  Their goal is to develop a framework, represented by Symbolic Propagation Layers (SPLs), that allows for the seamless incorporation of logical constraints and probabilistic reasoning capabilities into neural network architectures. By doing so, they aim to improve the performance of neural network classifiers on challenging tasks that involve both symbolic reasoning and probabilistic decision-making.",
	"motivations_priorities": "The authors' primary goals and motivations revolve around addressing the limitations of deep learning models, particularly in structured output prediction (SOP) tasks where predictions need to adhere to logical constraints. They highlight the challenges of ensuring that deep learning models produce predictions consistent with domain-specific logical constraints, which is crucial in various safety-critical scenarios such as protein function prediction and drug discovery. Existing approaches either fail to guarantee consistency with constraints at inference time or are restricted to specific types of symbolic knowledge. Therefore, the authors propose a novel Semantic Probabilistic Layer (SPL) that can be seamlessly integrated into neural networks to ensure predictions comply with logical constraints while maintaining modularity, expressivity, and efficiency. Their contributions include identifying key requirements for neuro-symbolic predictors, introducing SPL, and demonstrating its effectiveness in challenging SOP tasks compared to state-of-the-art approaches.",
	"eval_metrics": " The evaluation of the system is quite comprehensive and involves testing on various neuro-symbolic benchmarks, including tasks like simple path prediction, preference learning, shortest path finding in Warcraft, and hierarchical multi-label classification (HMLC).  For simple path prediction and preference learning tasks, they use neural networks to predict paths or user preferences. They compare their approach, called SPL (Symbolic Propagation Layers), against baseline methods such as a multi-layer perceptron (MLP) equipped with feature interaction layers (FIL) and semantic loss (LSL). They measure performance using metrics like exact match, Hamming score, and consistency.  In the case of predicting the minimum cost path in Warcraft, they compare SPL against FIL and optionally LSL. They use ResNet18 as the base architecture and evaluate performance based on how close the predicted paths are to the ground truth in terms of cost.  For hierarchical multi-label classification tasks, they evaluate SPL against HMCNN (Hierarchical Multi-Label Convolutional Neural Network). They compare performance in terms of exact match on various datasets spanning different domains.  Overall, SPL consistently outperforms competitors across different tasks while ensuring consistent predictions. They also provide insights into the computational efficiency of SPL compared to other methods.",
	"approach": "The authors' primary approach involves designing a Semantic Probabilistic Layer (SPL) for neuro-symbolic Structured Output Prediction (SOP) tasks. They define SOP as tasks where a neural network classifier must learn to associate instances with interdependent labels, and they abstract the neural classifier into a feature extractor and a predictive final layer. The final layer in many existing classifiers assumes conditional independence among labels given the features, leading to suboptimal results, especially when logical constraints among labels exist. To address this, they introduce SPL, which seamlessly integrates into neural networks and ensures predictions conform to logical constraints while maintaining efficiency, expressivity, and differentiability. SPL combines probabilistic inference with logical reasoning using smooth and decomposable circuits, enabling efficient inference and training while guaranteeing consistency with the specified constraints. Their approach satisfies six desiderata for neuro-symbolic SOP: probabilistic semantics, expressiveness, consistency with logical constraints, support for general constraints, modularity, and efficiency. They provide a comprehensive framework for implementing SPLs, including representing expressive distributions with probabilistic circuits and encoding logical formulas with constraint circuits. Additionally, they propose a single-circuit implementation of SPL for improved efficiency and flexibility, showcasing the effectiveness of their approach in various neuro-symbolic SOP tasks compared to existing methods.",
	"results": "The results of the work demonstrate that SPLs (Symbolic Propagation Layers) provide a novel framework for integrating complex probabilistic reasoning and logical constraints into neural network classifiers while maintaining efficient inference and training. They achieve noticeable improvements over the current state-of-the-art on challenging neuro-symbolic benchmarks such as pathfinding and hierarchical multi-label classification (HMLC).",
	"future_work": "This success opens up several intriguing research directions. First, SPLs can be extended to incorporate logical constraints across multiple networks, potentially represented by first-order formulas. This extension could make the circuit construction process transparent to users and possibly allow for the automatic learning of constraints from data, enhancing the flexibility and adaptability of the approach.  Second, there is interest in leveraging SPLs to introduce scalable logical constraints into large language models. By integrating SPLs into language models, it could enable these models to perform probabilistic reasoning, which could have significant implications for various natural language processing tasks. These potential research directions highlight the versatility and potential impact of SPLs in advancing neuro-symbolic approaches in machine learning."
},
{
	"metadata" : 
	{
		"title": "NS3: Neuro-Symbolic Semantic Code Search",
		"year": 2022,
		"month": 0,
		"authors": [
			{
				"name": "Shushan Arakelyan"
			},
			{
				"name": "Anna Hakhverdyan"
			},
			{
				"name": "Miltiadis Allamanis"
			},
			{
				"name": "Luis Garcia"
			},
			{
				"name": "Christophe Hauser"
			},
			{
				"name": "Xiang Ren"
			}
		],
		"abstract": "Semantic code search is the task of retrieving a code snippet given a textual description of its functionality. Recent work has been focused on using similarity metricsbetween neural embeddings of text and code. However, current language models are known to struggle with longer, compositional text, and multi-step reasoning. To overcome this limitation, we propose supplementing the query sentence with a layout of its semantic structure. The semantic layout is used to break down the final reasoning decision into a series of lower-level decisions. We use a Neural Module Network architecture to implement this idea. We compare our model - NS3 (Neuro-Symbolic Semantic Search) - to a number of baselines, including state-of-the-art semantic code retrieval methods, and evaluate on two datasets - CodeSearchNet and Code Search and Question Answering. We demonstrate that our approach results in more precise code retrieval, and we study the effectiveness of our modular design when handling compositional queries.",
		"keywords": "",
		"institutions": [
			{
				"name": "University of Southern California, Department of Computer Science"
			},
			{
				"name": "National Polytechnic University of Armenia"
			},
			{
				"name": "Microsoft Research Cambridge"
			},
			{
				"name": "USC Information Sciences Institute"
			}
		]
	},
	"overview": "This research introduces NS3, a novel approach to semantic code search using neural module networks (NMNs). NS3 represents natural language queries and code snippets in terms of actions and data entities, constructing a neural module network based on the semantic structure of the query. Through extensive evaluation, NS3 outperforms strong baseline methods, demonstrating its effectiveness in capturing the nuanced meaning of queries for accurate code retrieval. Additionally, the study explores the model's generalization capabilities, robustness, and sensitivity to query perturbations, paving the way for further advancements in semantic code search.",
	"critical_concepts": "Semantic Code Search: The primary focus of the research is on semantic code search, which involves retrieving code snippets based on the meaning or intent of natural language queries rather than just keyword matching. Neural Module Networks (NMNs): The proposed method, NS3, is based on neural module networks. NMNs decompose complex tasks into modular components, each responsible for handling specific aspects of the task. These modules are then composed to form a network that can execute the overall task. Natural Language Processing (NLP): Since the queries are in natural language, a strong understanding of NLP techniques is essential. This includes parsing natural language queries, understanding semantic structures, and mapping them to code. Pretrained Language Models: The research leverages pretrained language models, such as RoBERTa and CodeBERT, which are pretrained on large-scale text corpora to capture contextual information and semantic representations. Evaluation Metrics: The research evaluates the performance of the proposed method using metrics such as Mean Reciprocal Rank (MRR) and Precision@K (P@K), which are common in information retrieval tasks. Experimentation and Analysis: The research involves conducting experiments on datasets, performing ablation studies to understand the impact of different components, and analyzing the results to draw insights and conclusions.",
	"domain_task_problem": " The researchers are focused on solving the task of semantic code search. Specifically, they aim to develop a method that allows developers to search for code snippets based on the semantic meaning of their natural language queries. This task is essential for software development, as developers often need to find relevant code examples or functions to reuse in their projects. By enabling semantic code search, the researchers aim to improve developers' productivity and efficiency by providing them with more accurate and relevant search results based on the intended functionality rather than just keyword matching.",
	"motivations_priorities": "The primary goal of the authors is to address the limitations of current neural approaches in semantic code search (SCS), particularly in dealing with a wide range of natural-language queries, especially those with compositional language structures. They propose a modular workflow based on the semantic structure of the query, inspired by how an engineer would approach an SCS task. The approach involves breaking down the task of matching the query into smaller tasks of matching individual data entities and actions, and tackling each part with a distinct type of network called a neural module. By constructing the layout of how modules' outputs should be linked according to the relationships between data entities and actions, they aim to mimic staged reasoning necessary for SCS. The proposed model, called NS3, is evaluated on SCS datasets and compared against baseline models, demonstrating significant improvements in sensitivity to semantic changes in queries and the ability to handle compositional queries effectively.",
	"eval_metrics": "The authors evaluate their research using Mean Reciprocal Rank (MRR) and Precision@K (P@K) metrics, where K is set to 1, 3, and 5. They conduct experiments on two datasets: the Python portion of the CodeSearchNet (CSN) dataset and the CoSQA dataset. For evaluation, they follow CodeSearchNet's original approach, comparing the output against outputs over a fixed set of 999 distractor code snippets. In their evaluation, they use a two-step process: first, obtaining CodeBERT's output against 999 distractors, and second, using their proposed NS3 model to re-rank the top 10 predictions of CodeBERT. This approach enables faster evaluation. They compare their NS3 model with various state-of-the-art methods, including traditional approaches for document retrieval and pretrained large NLP language models. Additionally, they perform ablation studies to analyze the impact of different factors on the model's performance, such as query complexity and training variations. The results show that their proposed NS3 model outperforms the baselines across different evaluation metrics and datasets, indicating the effectiveness of their approach for semantic code search.Additionally, they perform ablation studies to analyze the impact of different factors on the model's performance. These studies include:  Effect of Pretraining: Comparing the performance of the original model with versions where part of the pretraining is skipped. Score Normalization: Investigating the importance of output normalization for the modules to achieve a proper probability distribution. Similarity Metric: Experimenting with different similarity metrics, such as dot product similarity, L2 distance, and weighted cosine similarity. Case Study: Demonstrating examples of the scores produced by the modules at different stages of training. Perturbed Query Evaluation: Studying how sensitive the models are to small changes in the query, which no longer correctly describes its corresponding code snippet. Through these experiments and evaluations, the researchers aim to assess the effectiveness and robustness of their proposed method compared to baseline models and to gain insights into the behavior of their model under various conditions.",
	"approach": "The authors propose an approach for semantic code search (SCS) that involves supplementing the query with a loose structure resembling its semantic parse. This structure is used to break down the query into smaller, semantically coherent parts, each corresponding to an individual execution step. The approach employs a neural module network composed of different types of modules, including entity discovery modules and action modules. Entity discovery modules estimate the semantic relatedness of code tokens to entities mentioned in the query, while action modules estimate the likelihood of code tokens being related to unseen entities affected by actions in the query. The modules are nested based on the semantic parse layout, and their outputs are combined to predict whether the code snippet matches the query. During training, the model undergoes noisy supervision pretraining for both modules, followed by end-to-end training using positive and negative examples. The layout of the neural module network is dynamically constructed based on the semantic structure of the query, and during inference, the model instantiates a network based on this layout to predict the match between the query and code snippet. This approach aims to improve the sensitivity to semantic changes in queries and effectively handle compositional queries in SCS.",
	"results": "The results of the study indicate that NS3, a symbolic method for semantic code search based on neural module networks, outperforms strong but unstructured baselines in an extensive evaluation. By representing the query and code in terms of actions and data entities and utilizing the semantic structure of the query to construct a neural module network, NS3 more precisely captures the nature of queries compared to existing code search methods.",
	"future_work": "Improved Parsing: Given that their method heavily relies on parsing natural language queries into symbolic representations, enhancing the performance of the parsing component could lead to better overall performance. This could involve exploring more sophisticated parsing techniques or leveraging external resources to improve parsing accuracy. Generalization and Robustness: Investigating methods to improve the generalization capabilities and robustness of NS3 could be valuable. This could involve exploring techniques to handle completely unseen actions and entities, as well as analyzing the impact of the frequency of observing an action or entity during training on model performance. Model Interpretability: Enhancing the interpretability of the NS3 model could provide insights into how it makes decisions and which parts of the query and code it focuses on during the search process. This could involve developing techniques to visualize the attention mechanisms or intermediate representations of the model. Extension to Other Programming Languages: Extending NS3 to support code search in programming languages other than Python could broaden its applicability and impact. This would involve adapting the model architecture and training procedures to accommodate the characteristics of different programming languages. Integration with Development Tools: Integrating NS3 into existing development tools and workflows could streamline the code search process for developers. This could involve developing plugins or APIs that allow developers to interact with NS3 directly within their preferred development environments. Exploration of Alternative Architectures: Exploring alternative neural architectures or learning paradigms for semantic code search could lead to further improvements in performance and efficiency. This could involve investigating techniques such as graph neural networks or reinforcement learning for code search tasks."
}
]